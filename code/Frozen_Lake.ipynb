{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98420a01",
   "metadata": {},
   "source": [
    "# Customizing OpenAI Gym Environments and Implementing Reinforcement Learning Agents with Stable Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e669eb5",
   "metadata": {},
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import A2C, PPO,DQN\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "from sb3_contrib.ars import ARS\n",
    "from sb3_contrib.qrdqn import QRDQN\n",
    "from sb3_contrib.trpo import TRPO\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from gymnasium import Wrapper\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2096c298",
   "metadata": {},
   "source": [
    "### **Frozen Lake**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741a1d61",
   "metadata": {},
   "source": [
    "After exploring the existing environments, we decided to pick Frozen Lake. \n",
    "\n",
    "\n",
    "#### **Analysing the chosen environment**\n",
    "\n",
    "This environment is part of the Toy Text environments and consists of a 4X4 grid by default.\n",
    "\n",
    "- ##### **Action Space**:\n",
    "\n",
    "   The action space is Discrete(4) and its shape is (1,) wich indicates that there are four possible actions the player can make.\n",
    "\n",
    "   These are in the range {0,3} and are: \n",
    "   - 0 - Move up, \n",
    "   - 1 - Move right\n",
    "   - 2 - Move down\n",
    "   - 3 - Move left\n",
    "\n",
    "\n",
    "- ##### **Observation Space**:\n",
    "\n",
    "   The observation space consists in the value of the player's position. It starts at 0 and when using the 4X4 map, it goes up to 15, which corresponds to the goal state. \n",
    "\n",
    "   The expression to calculate the the actual position is ``current_row * number_of_rows + number_of_columns.``\n",
    "\n",
    "- ##### **Starting state**:\n",
    "\n",
    "   The episode starts with the player in the 0 position, being the corresponding rows and columns both 0.\n",
    "\n",
    "- ##### **Rewards**:\n",
    "\n",
    "   There are three different tiles in the board and the rewards consist in reaching them:\n",
    "   - Reach goal: +1 reward\n",
    "   - Reach hole: 0 reward\n",
    "   - Reach frozen tile: 0 reward\n",
    "\n",
    "- ##### **End of the episode**:\n",
    "\n",
    "   The episode has two different forms of ending:\n",
    "   - Termination: When player reaches a hole or the goal (always located at the last column and row).\n",
    "   - Truncation: When player reaches the time_limit_wrapper, being 100 for 4X4 board and 200 for 8X8.\n",
    "\n",
    "- ##### **Information**:\n",
    "\n",
    "   The step() function returns a dictionary with 5 arguments:\n",
    "   - observation (int)\n",
    "   - reward (int)\n",
    "   - end_of_the_episode (bol)\n",
    "   - truncation (bol)\n",
    "   - probability_of_transition (dictionary)\n",
    "\n",
    "   The reset() functions returns:\n",
    "   - observation for the initial state (int)\n",
    "   - probability_of_transition (dictionary):\n",
    "      - {\"prob\": 1}   - if the slippery is False\n",
    "      - {\"prob\": 1/3} - if the slippery is True\n",
    "\n",
    "- ##### **Arguments**:\n",
    "\n",
    "   gym.make() can receive some arguments: \n",
    "   - desc - if None the map is a non custom map or it can generate a random map with a certain size or it can be a list of strings that specify a custom map. \n",
    "      - Example: desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"].\n",
    "   - map_name - ID to use a certain preloaded map.\n",
    "   - is_slippery - if true then there is 1/3 the player will move in the intended direction\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b366f684",
   "metadata": {},
   "source": [
    "### **Testing the random agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0481e33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False, render_mode=\"human\")\n",
    "env.reset() \n",
    "\n",
    "episodes = 10  \n",
    "for ep in range(episodes):\n",
    "    env.reset()  \n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()  \n",
    "        obs, rewards, done, truncated, prob = env.step(env.action_space.sample())\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bc137f",
   "metadata": {},
   "source": [
    "As we can see the agent performs very poorly when playing randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522d270f",
   "metadata": {},
   "source": [
    "### **Training models chosen**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24ab174",
   "metadata": {},
   "source": [
    "After checking the reinforcement learning models that were compatible with our environment we decided to test all of the following:\n",
    "- Advantage Actor Critic (A2C)\n",
    "- Proximal Policy Optimization (PPO)\n",
    "- Deep Q Network (DQN)\n",
    "- Trust Region Policy Optimization (TRPO)\n",
    "- Quantile Regression DQN (QR-DQN)\n",
    "- Maskable PPO\n",
    "- Augmented Random Search (ARS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd6997c",
   "metadata": {},
   "source": [
    "### **Modifications**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d39424",
   "metadata": {},
   "source": [
    "After having in mind the information above we noticed that the reward system is sparse, meaning that the rewards are only attributed when achieving a significant milestone, in this case the goal. We intend to  change it to a dense one in order to provide enough feedback for the agent to reach the optimal solution, minimizing the path to the goal and also penalising when the agent moves into a hole. Since we are altering those rewards we will also need to increase the amount when getting to the final state.\n",
    "The initial idea is to take away 1 of reward for each step, because as is, the agent is not taking an optimal path and can sometimes perform repeated actions that do not lead to the solution, leading to unwanted behaviour.\n",
    "When moving into a hole, the reward need to be a big negative number so that taking a certain number os steps does not equal reaching a hole, since the first one still has the possibility of getting to the goal. We will try -100.\n",
    "Since we are penalising the agent so much, we will try to increase the reward to 100 when getting to the final state. The reasoning is that we want to encourage finding the solution and not only taking few steps, since the reward would be greater.\n",
    "Other modification we can implement is turning off the freezing tiles,  that way the randomness factor is not taken in consideration in this problem, and we can compare the two scenarios and see how it impacts the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19812db8",
   "metadata": {},
   "source": [
    "### **Implement customizations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0d04eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRewardWrapper(Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super(CustomRewardWrapper, self).__init__(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncation, info = self.env.step(action)\n",
    "\n",
    "        if done and obs != 15:\n",
    "            reward = -100\n",
    "        elif obs == 15:\n",
    "            reward = 100\n",
    "        else:\n",
    "            reward = -1\n",
    "        return obs, reward, done, truncation, info\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"human\")\n",
    "env = CustomRewardWrapper(env)  \n",
    "env.reset()   \n",
    "\n",
    "episodes = 10  \n",
    "for ep in range(episodes):\n",
    "    env.reset()  \n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        obs, rewards, done, truncated, prob = env.step(env.action_space.sample())\n",
    "        print(\"reward \", rewards)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f697a09e",
   "metadata": {},
   "source": [
    "After rechearching about the models we came to the conclusion that some of them can be sensitive to rewards in a large range so we decided to change them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7171d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomReward(Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super(CustomReward, self).__init__(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncation, info = self.env.step(action)\n",
    "\n",
    "        if done and obs != 15:\n",
    "            reward = -10\n",
    "        elif obs == 15:\n",
    "            reward = 1.6\n",
    "        else:\n",
    "            reward = -0.1\n",
    "        return obs, reward, done, truncation, info\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"human\")\n",
    "env = CustomReward(env)  \n",
    "env.reset()   \n",
    "\n",
    "episodes = 10  \n",
    "for ep in range(episodes):\n",
    "    env.reset()  \n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        obs, rewards, done, truncated, prob = env.step(env.action_space.sample())\n",
    "        print(\"reward \", rewards)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c577cab0",
   "metadata": {},
   "source": [
    "### **Without Slippery**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881a411c",
   "metadata": {},
   "source": [
    "#### **Training all models - (500 000 timesteps)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56463b4d",
   "metadata": {},
   "source": [
    "We are going to train all the chosen models with a small number of timesteps to check the ones that perform the best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393bac17",
   "metadata": {},
   "source": [
    "##### **A2C**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f94926",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False, render_mode=\"rgb_array\")\n",
    "env = CustomReward(env)\n",
    "env.reset()\n",
    "\n",
    "log_path = os.path.join('Training','WithChange','SlipperyOff','Agent','Logs')\n",
    "model = A2C('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "\n",
    "TIMESTEPS = 500000\n",
    "model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"A2C\")\n",
    "model.save(f\"Training/WithChange/SlipperyOff/Agent/models/A2C/{TIMESTEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf2e7f4",
   "metadata": {},
   "source": [
    "###### **Image**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a27a31",
   "metadata": {},
   "source": [
    "![Image](images/SlipperyOff/Agents/A2C.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cf330c",
   "metadata": {},
   "source": [
    "##### **PPO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c960de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False, render_mode=\"rgb_array\")\n",
    "env = CustomReward(env)\n",
    "env.reset()\n",
    "\n",
    "log_path = os.path.join('Training','WithChange','SlipperyOff','Agent','Logs')\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "\n",
    "TIMESTEPS = 500000\n",
    "model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"PPO\")\n",
    "model.save(f\"Training/WithChange/SlipperyOff/Agent/models/PPO/{TIMESTEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6f6a0a",
   "metadata": {},
   "source": [
    "###### **Image**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc08d0cd",
   "metadata": {},
   "source": [
    "![Image](images/SlipperyOff/Agents/PPO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be29f1c",
   "metadata": {},
   "source": [
    "##### **DQN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9868ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False, render_mode=\"rgb_array\")\n",
    "env = CustomReward(env)\n",
    "env.reset()\n",
    "\n",
    "log_path = os.path.join('Training','WithChange','SlipperyOff','Agent','Logs')\n",
    "model = DQN('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "\n",
    "TIMESTEPS = 500000\n",
    "model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"DQN\")\n",
    "model.save(f\"Training/WithChange/SlipperyOff/Agent/models/DQN/{TIMESTEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354cf8cb",
   "metadata": {},
   "source": [
    "###### **Image**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f96434",
   "metadata": {},
   "source": [
    "![Image](images/SlipperyOff/Agents/DQN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644d0de7",
   "metadata": {},
   "source": [
    "##### **TRPO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcbc590",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False, render_mode=\"rgb_array\")\n",
    "env = CustomReward(env)\n",
    "env.reset()\n",
    "\n",
    "log_path = os.path.join('Training','WithChange','SlipperyOff','Agent','Logs')\n",
    "model = TRPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "\n",
    "TIMESTEPS = 500000\n",
    "model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"TRPO\")\n",
    "model.save(f\"Training/WithChange/SlipperyOff/Agent/models/TRPO/{TIMESTEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc75d83f",
   "metadata": {},
   "source": [
    "###### **Image**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7316192d",
   "metadata": {},
   "source": [
    "![Image](images/SlipperyOff/Agents/TRPO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472deb0b",
   "metadata": {},
   "source": [
    "##### **QR-DQN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c4480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False, render_mode=\"rgb_array\")\n",
    "env = CustomReward(env)\n",
    "env.reset()\n",
    "\n",
    "log_path = os.path.join('Training','WithChange','SlipperyOff','Agent','Logs')\n",
    "model = QRDQN('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "\n",
    "TIMESTEPS = 500000\n",
    "model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"QRDQN\")\n",
    "model.save(f\"Training/WithChange/SlipperyOff/Agent/models/QRDQN/{TIMESTEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c33c2e",
   "metadata": {},
   "source": [
    "###### **Image**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb1b238",
   "metadata": {},
   "source": [
    "![Image](images/SlipperyOff/Agents/QRDQN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7836ec3e",
   "metadata": {},
   "source": [
    "##### **Maskable PPO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5641158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_fn(env):\n",
    "    return [True, True, True, False]\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"rgb_array\")\n",
    "env = ActionMasker(env,mask_fn)\n",
    "env = CustomReward(env)\n",
    "\n",
    "log_path = os.path.join('Training','WithChange','SlipperyOff','Agent','Logs')\n",
    "model = MaskablePPO(MaskableActorCriticPolicy, env, verbose=1, tensorboard_log=log_path)\n",
    "\n",
    "TIMESTEPS = 500000\n",
    "model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"Maskable PPO\")\n",
    "model.save(f\"Training/WithChange/SlipperyOff/Agent/models/Maskable PPO/{TIMESTEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4518dca3",
   "metadata": {},
   "source": [
    "###### **Image**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494f3c41",
   "metadata": {},
   "source": [
    "![Image](images/SlipperyOff/Agents/MaskablePPO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0e4576",
   "metadata": {},
   "source": [
    "##### **ARS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aefadf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False, render_mode=\"rgb_array\")\n",
    "env = CustomReward(env)\n",
    "env.reset()\n",
    "\n",
    "log_path = os.path.join('Training','WithChange','SlipperyOff','Agent','Logs')\n",
    "model = ARS('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "\n",
    "TIMESTEPS = 500000\n",
    "model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"ARS\")\n",
    "model.save(f\"Training/WithChange/SlipperyOff/Agent/models/ARS/{TIMESTEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882ae5aa",
   "metadata": {},
   "source": [
    "###### **Image**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c2923f",
   "metadata": {},
   "source": [
    "![Image](images/SlipperyOff/Agents/ARS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35315c0",
   "metadata": {},
   "source": [
    "#### **Choosing the 3 best ones**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272c36b8",
   "metadata": {},
   "source": [
    "By looking at the graphs we can see that the number of timesteps was enough for the majority of models to stabilize their learning curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbfc344",
   "metadata": {},
   "source": [
    "![Image](images/SlipperyOff/Agents/TODOS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc12dfa3",
   "metadata": {},
   "source": [
    "In the mean reward graph above we can see that tree models got an average reward of 1.1 or close, which was the max reward possible for each episode, which means they learn how to play the agent very effectivly when the randomness is turned off.\n",
    "\n",
    "Based on the results we decided to pick the 3 best performing models to train with more timesteps:\n",
    "- Advantage Actor Critic (A2C)\n",
    "- Proximal Policy Optimization (PPO)\n",
    "- Trust Region Policy Optimization (TRPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c820c6f",
   "metadata": {},
   "source": [
    "#### **Training the 3 best - (5 000 000 timesteps)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e9fa76",
   "metadata": {},
   "source": [
    "Now we are going to train more deeply the chosen models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2b5af2",
   "metadata": {},
   "source": [
    "##### **A2C**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1418c345",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False, render_mode=\"rgb_array\")\n",
    "env = CustomReward(env)\n",
    "env.reset()\n",
    "\n",
    "log_path = os.path.join('Training','WithChange''SlipperyOff','Top3_Agent','Logs')\n",
    "model = A2C('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "\n",
    "TIMESTEPS = 10000\n",
    "iters = 1\n",
    "while TIMESTEPS*iters < 5000000:\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"A2C\")\n",
    "    model.save(f\"Training/WithChange/SlipperyOff/Top3_Agent/models/A2C/{iters*TIMESTEPS}\")\n",
    "    iters+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6d7ce8",
   "metadata": {},
   "source": [
    "###### **Image**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01a4703",
   "metadata": {},
   "source": [
    "![Image](images/SlipperyOff/Top3_Agent/A2C.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2e67c7",
   "metadata": {},
   "source": [
    "##### **PPO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca6be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False, render_mode=\"rgb_array\")\n",
    "env = CustomReward(env)\n",
    "env.reset()\n",
    "\n",
    "log_path = os.path.join('Training','WithChange','SlipperyOff','Top3_Agentt','Logs')\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "\n",
    "TIMESTEPS = 10000\n",
    "iters = 1\n",
    "while TIMESTEPS*iters < 5000000:\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"PPO\")\n",
    "    model.save(f\"Training/WithChange/SlipperyOff/Top3_Agent/models/PPO/{TIMESTEPS*iters}\")\n",
    "    iters += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bc7478",
   "metadata": {},
   "source": [
    "###### **Image**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58dba7e",
   "metadata": {},
   "source": [
    "![Image](images/SlipperyOff/Top3_Agent/PPO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38cda4e",
   "metadata": {},
   "source": [
    "##### **TRPO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269b2081",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1',is_slippery=False, render_mode=\"rgb_array\")\n",
    "env = CustomReward(env)\n",
    "env.reset()\n",
    "\n",
    "log_path = os.path.join('Training','WithChange','SlipperyOff','Top3_Agent','Logs')\n",
    "model = TRPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "\n",
    "TIMESTEPS = 10000\n",
    "iters = 1\n",
    "while TIMESTEPS*iters < 5000000:\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"TRPO\")\n",
    "    model.save(f\"Training/WithChange/SlipperyOff/Top3_Agent/models/TRPO/{TIMESTEPS*iters}\")\n",
    "    iters += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8425b792",
   "metadata": {},
   "source": [
    "###### **Image**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d724712b",
   "metadata": {},
   "source": [
    "![Image](images/SlipperyOff/Top3_Agent/TRPO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b68e20",
   "metadata": {},
   "source": [
    "#### **Choosing the best performer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023ebca7",
   "metadata": {},
   "source": [
    "![Image](images/SlipperyOff/Top3_Agent/TODOS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35afcf9c",
   "metadata": {},
   "source": [
    "After trainning our models with 5 million timesteps, we analised the graph and got to the conclusion that the A2C is the better one in this case, since it stabilizes at the top, which the PPO does not, and gets that first comparing to the TRPO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f387ad51",
   "metadata": {},
   "source": [
    "#### **Testing the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62ee353",
   "metadata": {},
   "source": [
    "With the following code we will see how the agent performs based on the prediction of the A2C model. We will run 10 episodes using the model trained with 500 thousand since by then it already stabilized with the max reward and the lenth of the episode was the minimum. \n",
    "\n",
    "![Image](images/SlipperyOff/Top3_Agent/A2C.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9647d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', render_mode=\"human\")\n",
    "env = CustomReward(env)\n",
    "env.reset()\n",
    "\n",
    "models_dir = \"Training/WithChange/SlipperyOff/Top3_Agent/models/A2C\"\n",
    "model_path = f\"{models_dir}/4000000.zip\"\n",
    "\n",
    "model = A2C.load(model_path, env=env)\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "for ep in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, truncated, prob = env.step(action.item())\n",
    "        print(reward)\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695b0c2e",
   "metadata": {},
   "source": [
    "The agent got to the goal in all episodes. In total it made 60 steps, on average 6 steps per episode that corresponds to the optimal solution. The mean reward was 1 and was the max possible. We can conclude that the model learnt how to play the game perfectly which is no surprise as the environment never changes and it only needs to do the same actions in every episode. It is also worth noticing that the number of timesteps it took was rather small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a93671",
   "metadata": {},
   "source": [
    "#### **Comparison with original environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715c5077",
   "metadata": {},
   "source": [
    "In both environments the agents gets to goal in all simulations with the optimal solution. The only noticeable difference is in the graphs of mean lenght reward however it does not have an impact on the agent's behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9fa9f8",
   "metadata": {},
   "source": [
    "![Image](images\\NoChange\\A2C.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae95fc7",
   "metadata": {},
   "source": [
    "#### **Hyperparameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1564069e",
   "metadata": {},
   "source": [
    "The only thing that can improve relatively to the previous model is number of iterations needed to get to minimum of steps. We applied some hyperparameters to make it learn faster: \n",
    " - learning rate: 0.01\n",
    " - gamma: 1\n",
    "\n",
    "We used the following script to train and ran it for 10 million timesteps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9cf620",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', render_mode=\"rgb_array\")\n",
    "env = CustomReward(env)\n",
    "env.reset()\n",
    "\n",
    "log_path = os.path.join('Training','Hyperparameter','Logs')\n",
    "model = A2C('MlpPolicy', env, verbose=1, learning_rate=0.01, gamma=1, tensorboard_log=log_path)\n",
    "\n",
    "TIMESTEPS = 10000\n",
    "iters = 1\n",
    "while TIMESTEPS*iters < 10000000:\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"A2C\")\n",
    "    model.save(f\"Training/Hyperparameter/models/A2C/{iters*TIMESTEPS}\")\n",
    "    iters+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af90c7cf",
   "metadata": {},
   "source": [
    "#### **Evaluate**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf07bdf7",
   "metadata": {},
   "source": [
    "Since the path to solution is always the same there is no need to evaluate extensively the performance of the model with hyperparameters since it is not possible to have a better result than the one we already have with a certain number of timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f888cb5",
   "metadata": {},
   "source": [
    "### **With Slippery**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55803791",
   "metadata": {},
   "source": [
    "#### **Training all models - (500 000 timesteps)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686ad7ba",
   "metadata": {},
   "source": [
    "We are going to train all the chosen models with a small number of timesteps to check the ones that perform the best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5898f4",
   "metadata": {},
   "source": [
    "##### **A2C**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4490f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', render_mode=\"rgb_array\")\n",
    "env = CustomReward(env)\n",
    "env.reset()\n",
    "\n",
    "log_path = os.path.join('Training','WithChange','SlipperyOn','Agent','Logs')\n",
    "model = A2C('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "\n",
    "TIMESTEPS = 500000\n",
    "model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"A2C\")\n",
    "model.save(f\"Training/WithChange/SlipperyOn/Agent/models/A2C/{TIMESTEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802e7ae9",
   "metadata": {},
   "source": [
    "###### **Image**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298d8485",
   "metadata": {},
   "source": [
    "![Image](images/SlipperyOn/Agents/A2C.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c59ebd1",
   "metadata": {},
   "source": [
    "##### **PPO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d241a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', render_mode=\"rgb_array\")\n",
    "env = CustomReward(env)\n",
    "env.reset()\n",
    "\n",
    "log_path = os.path.join('Training','WithChange','SlipperyOn','Agent','Logs')\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "\n",
    "TIMESTEPS = 500000\n",
    "model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"PPO\")\n",
    "model.save(f\"Training/WithChange/SlipperyOn/Agent/models/PPO/{TIMESTEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e7f458",
   "metadata": {},
   "source": [
    "###### **Image**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4babfa19",
   "metadata": {},
   "source": [
    "![Image](images/SlipperyOn/Agents/PPO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04936915",
   "metadata": {},
   "source": [
    "##### **DQN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0980cd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', render_mode=\"rgb_array\")\n",
    "env = CustomReward(env)\n",
    "env.reset()\n",
    "\n",
    "log_path = os.path.join('Training','WithChange','SlipperyOn','Agent','Logs')\n",
    "model = DQN('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "\n",
    "TIMESTEPS = 500000\n",
    "model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"DQN\")\n",
    "model.save(f\"Training/WithChange/SlipperyOn/Agent/models/DQN/{TIMESTEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4888be83",
   "metadata": {},
   "source": [
    "###### **Image**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5134ebf4",
   "metadata": {},
   "source": [
    "![Image](images/SlipperyOn/Agents/DQN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e32d217",
   "metadata": {},
   "source": [
    "##### **TRPO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d11f8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', render_mode=\"rgb_array\")\n",
    "env = CustomReward(env)\n",
    "env.reset()\n",
    "\n",
    "log_path = os.path.join('Training','WithChange','SlipperyOn','Agent','Logs')\n",
    "model = TRPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "\n",
    "TIMESTEPS = 500000\n",
    "model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"TRPO\")\n",
    "model.save(f\"Training/WithChange/SlipperyOn/Agent/models/TRPO/{TIMESTEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c59ee0d",
   "metadata": {},
   "source": [
    "###### **Image**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb106f1",
   "metadata": {},
   "source": [
    "![Image](images/SlipperyOn/Agents/TRPO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b226617b",
   "metadata": {},
   "source": [
    "##### **QR-DQN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34854e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', render_mode=\"rgb_array\")\n",
    "env = CustomReward(env)\n",
    "env.reset()\n",
    "\n",
    "log_path = os.path.join('Training','WithChange','SlipperyOn','Agent','Logs')\n",
    "model = QRDQN('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "\n",
    "TIMESTEPS = 500000\n",
    "model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"QRDQN\")\n",
    "model.save(f\"Training/WithChange/SlipperyOn/Agent/models/QRDQN/{TIMESTEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b310afc6",
   "metadata": {},
   "source": [
    "###### **Image**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9710d2",
   "metadata": {},
   "source": [
    "![Image](images/SlipperyOn/Agents/QRDQN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf0946a",
   "metadata": {},
   "source": [
    "##### **Maskable PPO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5e28de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_fn(env):\n",
    "    return [True, True, True, False]\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"rgb_array\")\n",
    "env = ActionMasker(env,mask_fn)\n",
    "env = CustomReward(env)\n",
    "\n",
    "log_path = os.path.join('Training','WithChange','SlipperyOn','Agent','Logs')\n",
    "model = MaskablePPO(MaskableActorCriticPolicy, env, verbose=1, tensorboard_log=log_path)\n",
    "\n",
    "TIMESTEPS = 500000\n",
    "model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"Maskable PPO\")\n",
    "model.save(f\"Training/WithChange/SlipperyOn/Agent/models/Maskable PPO/{TIMESTEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e496638e",
   "metadata": {},
   "source": [
    "###### **Image**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b117041f",
   "metadata": {},
   "source": [
    "![Image](images/SlipperyOn/Agents/MaskablePPO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748a9791",
   "metadata": {},
   "source": [
    "##### **ARS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f969fb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', render_mode=\"rgb_array\")\n",
    "env = CustomReward(env)\n",
    "env.reset()\n",
    "\n",
    "log_path = os.path.join('Training','WithChange','SlipperyOn','Agent','Logs')\n",
    "model = ARS('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "\n",
    "TIMESTEPS = 500000\n",
    "model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"ARS\")\n",
    "model.save(f\"Training/WithChange/SlipperyOn/Agent/models/ARS/{TIMESTEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a87b180",
   "metadata": {},
   "source": [
    "###### **Image**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f4d4e8",
   "metadata": {},
   "source": [
    "![Image](images/SlipperyOn/Agents/ARS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6baac21",
   "metadata": {},
   "source": [
    "#### **Choosing the 3 best ones**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37394e77",
   "metadata": {},
   "source": [
    "![Image](images/SlipperyOn/Agents/TODOS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3602ce16",
   "metadata": {},
   "source": [
    "In the mean reward graph above we can see that tree models got an average reward of 1.1 or close, which was the max reward possible for each episode, which means they learn how to play the agent very effectivly when the randomness is turned off.\n",
    "With the randomness on the models don't perform as well when it is turned off. We can see that the max mean reward was very close to -4.6, which is not a very good result since it has a lot of room for improvements.\n",
    "\n",
    "Based on the results we decided to pick the 3 best performing models to train with more timesteps:\n",
    "- Advantage Actor Critic (A2C)\n",
    "- Proximal Policy Optimization (PPO)\n",
    "- Trust Region Policy Optimization (TRPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7371fe",
   "metadata": {},
   "source": [
    "#### **Training the 3 best - (5 000 000 timesteps)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b01af45",
   "metadata": {},
   "source": [
    "##### **A2C**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b741fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', render_mode=\"rgb_array\")\n",
    "env = CustomReward(env)\n",
    "env.reset()\n",
    "\n",
    "log_path = os.path.join('Training','WithChange','SlipperyOn','Top3_Agent','Logs')\n",
    "model = A2C('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "\n",
    "TIMESTEPS = 10000\n",
    "iters = 1\n",
    "while TIMESTEPS*iters < 5000000:\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"A2C\")\n",
    "    model.save(f\"Training/WithChange/SlipperyOn/Top3_Agent/models/A2C/{iters*TIMESTEPS}\")\n",
    "    iters+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4365d37c",
   "metadata": {},
   "source": [
    "###### **Image**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bbe882",
   "metadata": {},
   "source": [
    "![Image](images/SlipperyOn/Top3_Agent/A2C.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7d35d9",
   "metadata": {},
   "source": [
    "##### **PPO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adf99bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', render_mode=\"rgb_array\")\n",
    "env = CustomReward(env)\n",
    "env.reset()\n",
    "\n",
    "log_path = os.path.join('Training','WithChange','SlipperyOn','Top3_Agent','Logs')\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "\n",
    "TIMESTEPS = 10000\n",
    "iters = 1\n",
    "while TIMESTEPS*iters < 5000000:\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"PPO\")\n",
    "    model.save(f\"Training/WithChange/SlipperyOn/Top3_Agent/models/PPO/{TIMESTEPS*iters}\")\n",
    "    iters += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c126c3",
   "metadata": {},
   "source": [
    "###### **Image**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de84168",
   "metadata": {},
   "source": [
    "![Image](images/SlipperyOn/Top3_Agent/PPO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60760de",
   "metadata": {},
   "source": [
    "##### **TRPO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1da3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', render_mode=\"rgb_array\")\n",
    "env = CustomReward(env)\n",
    "env.reset()\n",
    "\n",
    "log_path = os.path.join('Training','WithChange','SlipperyOn','Top3_Agent','Logs')\n",
    "model = TRPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "\n",
    "TIMESTEPS = 10000\n",
    "iters = 1\n",
    "while TIMESTEPS*iters < 5000000:\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"TRPO\")\n",
    "    model.save(f\"Training/WithChange/SlipperyOn/Top3_Agent/models/TRPO/{TIMESTEPS*iters}\")\n",
    "    iters += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c26b2ca",
   "metadata": {},
   "source": [
    "###### **Image**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5c83b7",
   "metadata": {},
   "source": [
    "![Image](images/SlipperyOn/Top3_Agent/TRPO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d654ffa",
   "metadata": {},
   "source": [
    "#### **Choosing the best performer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d14650",
   "metadata": {},
   "source": [
    "Based on the observations of the graph, we can see that the TRPO stays more consistently with higher mean reward compared to the other models, despite having highs and lows like the other ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4994afd0",
   "metadata": {},
   "source": [
    "#### **Testing the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca00560",
   "metadata": {},
   "source": [
    "With the following code we will see how the agent performs based on the prediction of the TRPO model. We will run 10 episodes using the model trained with 2.5 million since it got a mean reward that was close to the highest and stayed at the top long enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eadbd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "1.6\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "1.6\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "1.6\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "1.6\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "1.6\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "1.6\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "1.6\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "1.6\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "1.6\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "-0.1\n",
      "1.6\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', render_mode=\"human\")\n",
    "env = CustomReward(env)\n",
    "env.reset()\n",
    "\n",
    "models_dir = \"Training/WithChange/SlipperyOn/Top3_Agent/models/TRPO\"\n",
    "model_path = f\"{models_dir}/2500000.zip\"\n",
    "\n",
    "model = TRPO.load(model_path, env=env)\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "for ep in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, truncated, prob = env.step(action.item())\n",
    "        print(reward)\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9a4952",
   "metadata": {},
   "source": [
    "Based on the chosen model, the agent got sucessfully to the goal 9 times and fell into a hole once.\n",
    "The agent performed 335 total steps, which gives us a mean of 33.5 steps per episode. This number is a bit higher than the optimal one because the path is only 6 steps long, but since sometimes the direction of the movement is random, we cannot say it is a bad result.\n",
    "In this case the mean reward was -2.81, mainly because each step takes away -0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0c6707",
   "metadata": {},
   "source": [
    "#### **Comparison**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e5150",
   "metadata": {},
   "source": [
    "With no changes to the environment and slippery turned on, the agent wins 9 times out of 10 and makes 345 steps. The mean is 34.5 which is on average 1 step more than the environment with our custom wrapper. This does not represent a big change, but is still an improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b614d0c",
   "metadata": {},
   "source": [
    "#### **Hyperparameter**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3d7dbb",
   "metadata": {},
   "source": [
    "To fine tune our agent's performance we decided to experiemnt with learning rate and gamma, which is a discount factor used to determine the relative weight between future rewards and immediate ones.\n",
    "After testing some values for a few timesteps, we got to the conclusion that for our environment and the algorithm chosen the best values were:\n",
    " - learning rate: 0.01\n",
    " - gamma: 1\n",
    "\n",
    "We used the following script to train and ran it for 10 million timesteps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3766246",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', render_mode=\"rgb_array\")\n",
    "env = CustomReward(env)\n",
    "env.reset()\n",
    "\n",
    "log_path = os.path.join('Training','Hyperparameter','Logs')\n",
    "model = TRPO('MlpPolicy', env, verbose=1, learning_rate=0.01, gamma=1, tensorboard_log=log_path)\n",
    "\n",
    "TIMESTEPS = 10000\n",
    "iters = 1\n",
    "while TIMESTEPS*iters < 10000000:\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"TRPO\")\n",
    "    model.save(f\"Training/Hyperparameter/models/TRPO/{iters*TIMESTEPS}\")\n",
    "    iters+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0980745b",
   "metadata": {},
   "source": [
    "#### **Evaluate**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a92eb67",
   "metadata": {},
   "source": [
    "We will test the model 100 times to see if the alterations we made with hyperparameters were positive or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a087709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "A recompensa média foi -2.290000034123659\n",
      "O desvio padrão da recompensa foi 2.438626699031943\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', render_mode=\"rgb_array\")\n",
    "env = CustomReward(env)\n",
    "env.reset()\n",
    "\n",
    "# Carregar o modelo treinado\n",
    "models_dir = \"Training/WithChange/SlipperyOn/Top3_Agent/models/TRPO\"\n",
    "model_path = f\"{models_dir}/2500000.zip\"\n",
    "model = TRPO.load(model_path, env=env)\n",
    "\n",
    "# Avaliar o desempenho do modelo\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10, deterministic=True, render=False)\n",
    "\n",
    "print(f\"A recompensa média foi {mean_reward}\")\n",
    "print(f\"O desvio padrão da recompensa foi {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c710f0af",
   "metadata": {},
   "source": [
    "### **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabfcd54",
   "metadata": {},
   "source": [
    "In summary our results are positive as both of the models improve with the changes made, both the ones to the environment and the hyperparameters.\n",
    "When we compare to the agent playing randomly there is a huge difference since with training it is able to win consistentely and without taking a very long and unnecessary path.\n",
    "With slippery turned off the agent plays perfectly which was expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b1aa6c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
